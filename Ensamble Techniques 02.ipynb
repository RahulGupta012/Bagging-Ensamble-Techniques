{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eef93ed-f0ca-46c3-b3ca-097f8de63970",
   "metadata": {},
   "source": [
    "# Ensemble TechniquesAnd Its Types-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce8e44-c253-47e6-82e7-3498fb8029d9",
   "metadata": {},
   "source": [
    "_____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006be93-18c4-4e22-a46e-cc923d88e68b",
   "metadata": {},
   "source": [
    "# Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269754d9-faf5-4112-943c-d5f956fc4f83",
   "metadata": {},
   "source": [
    "Overfitting is basically occurs when the model has trined too much with the training data, in such a case it can have reduce their accuracy with the new data..ie..low Bias occurs. In such a case when there are high chance of a model to be overfit , Ensamble techniques are playing a wide role. Beacuse ensamble techniques are taking the multiple samples from dataset , and by the aggrigate output of various models predictions, it genralize the real output. As it takes the samples of data and using multiple models to train it, so the chances of overfitting is reduced for sure.\n",
    "\n",
    "\n",
    "Overfitting occurs when, \n",
    "- High accuracy of Training data = Low Bias\n",
    "- low accuracy of test data =  High Verience\n",
    "\n",
    "**Balance Model :** Ideally, one aims to have both low bias and low variance.\n",
    "- A model with high bias may not capture the underlying patterns in the data. It tends to be too simple and may underfit the training data.\n",
    "- A model with low bias and high variance may capture the noise in the training data and perform well on the training set but poorly on new data. \n",
    "\n",
    "\n",
    "so here are the some points how bagging is reduced overfitting in decision tress:\n",
    "\n",
    "**Bootstrap Sampling :** In bootstrap sampling , multiple samples of data has been randomely selected from the origanal datset. even there are the chances to be selecting the the same dataset, which is selected before. and there are also having the probability to drop some training data set , while selecting the samples. As this case it clear that model can't be overfit.\n",
    "\n",
    "**Training Multiple Trees:** As multiples samples has been made by datset from bootstrap sampling. and each sample a assinged with a new decision tree. Which means each sample has been trained their own way and by their own requirments. The way that each tree captures the noise and pattern are different. So the chances of overfiting are reduced at a high number.\n",
    "\n",
    "**Reducing Variance :** Overfitting is often associated with high variance, where the model is sensitive to small fluctuations in the training data.  By combining multiple models trained on different subsets, bagging reduces variance and provides a more stable and reliable prediction.\n",
    "\n",
    "**Rebustnes of Outliers :** Bagging can help us also to rebust the outliers as each sample are traines in a different tree, it may used to reducing the impact of outliers.\n",
    "\n",
    "\n",
    "           In summary, bagging reduces overfitting in decision trees by introducing diversity through bootstrap sampling and combining the predictions of multiple trees. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b05b33-c6ed-45ef-92a2-927d3da5dac6",
   "metadata": {},
   "source": [
    "# Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca4b41-632e-4278-8976-85912bf1199c",
   "metadata": {},
   "source": [
    "Bagging is a ensambled technique, which takes the agreegrate predictions of various models to make their own prediction. It this prediction is might be made by voating(in the case of classifier)  or avrage calculations(in the case of regresser).\n",
    "\n",
    "**What is base learners ?**\n",
    "\n",
    "Base learners refers to the individual models that constitute the ensemble. These base learners are trained independently on different subsets of the training data, and their predictions are combined to form the final prediction of the ensemble. Example; Svm , decision tree, KNN.\n",
    " \n",
    "       Now, we here illusarated the various factors of baggings with respect their advantages and disadvanatages;\n",
    "       \n",
    "       \n",
    "                                         \n",
    "                                         ADVANATAGES\n",
    "                                         \n",
    "                                         \n",
    "**Advanatge 01 : Diversity of Base Learners:**\n",
    "\n",
    "To get the maximum benifit of the bagging technique we should involved the maximum number of the base models. so our main model take the maximum essence from the data in respect to show the output, which is more precise and accurate. It is importaqnt to note that by using the maximum base models, we are not in a favour to use the model , which is not so important. we should always rembered our requarments while chossing the base models.\n",
    "**Example :** We should choose svm, decision tree, knn as per our requirments.\n",
    "\n",
    "**Advanatge 02 : Reduction of Overfitting:** \n",
    "\n",
    "When our model is prone to overfitting , then we should choose our base model accordingly in the bagging. It also conclude their result by the voating and avrage , which is going to help the reducing overfitting.\n",
    "**Exapmle :** We should choose Decision tree as in the bagging for each sample of data , there is a new decision tree.\n",
    "\n",
    "**Advantage 03 : Improved Stability:**\n",
    "\n",
    "Bagging is also helps us to improved the stability of the model as  It reduces the sensitivity to outliers or noise in the data, as the impact of individual base learners is dampened through averaging or voting.\n",
    "**Example ** : High veriendce decision tree.\n",
    "\n",
    "\n",
    "                                       \n",
    "                                       \n",
    "                                       Disadvantages\n",
    "                                       \n",
    "                                       \n",
    "**Disadvantage 01 : Increased Computational Cost**\n",
    "\n",
    "As begging is a big process and multiple models are used in a single model and worked for a time. Because of these factors the computational cost of the bagging is become to high.\n",
    "**Example: **Training a bagged ensemble of deep neural networks\n",
    "\n",
    "**Disadvantge 02 : Loss of Interpretability:**\n",
    "\n",
    "Bagging often leads to ensemble models that are more difficult to interpret compared to individual base learners. The combination of diverse models may sacrifice interpretability.\n",
    "**Example:** Bagging with a mix of decision trees, support vector machines, and neural networks.\n",
    "\n",
    "                                     \n",
    "                                       ***\n",
    "                                       \n",
    "                                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c9d09-29ec-4ead-b936-f137d867ca2f",
   "metadata": {},
   "source": [
    "# Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29a56e-edd7-4776-8365-3b60506988b5",
   "metadata": {},
   "source": [
    "Our aim behind choosing the begging is always to be get a better performance for our model. low bias and verience is something that we need to make our model accurate. And the bias and verience is directly depends on the selction of the base models for bagging. There are some factors which shows that How does the choice of base learner affect the bias-variance tradeoff in bagging :\n",
    "\n",
    "   **High-Variance Base Learners:** If the verience of our model is high , it directly mean that our model is overfitting prone. In such a case Bagging can play a vital role. as it makes their conclusion by averaging or combining multiple models trained on different subsets of the data,so it  gives the result as reduced overfitting. \n",
    "\n",
    "  **Low-Bias Base Learners :** Bagging tends to work well when the base learners have low bias. This is the averaging or combining process may not significantly improve the model's performance if the individual base learners have high bias.\n",
    "\n",
    "Base learners with low bias and high variance benefit more from bagging as it helps to smooth out the variance without introducing too much bias.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f4e96-d05f-4fd2-adb7-17f7ca9500f6",
   "metadata": {},
   "source": [
    "# Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261395aa-950f-4c0c-b3ee-e8d7487e3107",
   "metadata": {},
   "source": [
    "Yes..bagging is a ensemble technique which is used for both problems ..ie..classification or regression. The main difference in both while using the bagging , is the way or method of aggrigation and finding the output from the multiple predictions of base model. Here it is illuserated how bagging decises their final outcome in the case of reggresion or classification.\n",
    "\n",
    "  **Voating Method :** This method is basically used for classification method. In this method we select the final outcome by the most occuring prediction elements of the base model. For example in the Binary classification we are using 5 base models and in which 3 base models predict \"Yes\" as output and rest of the models predict \"NO\". So the final outcome of bagging is always being \"Yes\".\n",
    "  \n",
    "  **Avrage Method :** It is used for regression probelems as regression has the numerical values and which are continues so there are no possibility of the voating system. In such a case we are going for avrage value of the continues values as a final output of the bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f44ae4-bc9f-405d-8597-1dca65abe039",
   "metadata": {},
   "source": [
    "# Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d90e8-b0bd-4170-b424-fab2b46c1558",
   "metadata": {},
   "source": [
    "The ensembled size in bagging refers the number of base learners used in bagging. The ensamble size play a wide role in the performance of the ensemble. There are some factores which are influenced by the size of ensambles :\n",
    "\n",
    "  **Diversity is Good :** As the number of the base learners is increased or in other words when the size of ensamble is increased , the perforamnce of the model will also increased. As it takes the noise of the data precisly and reduced the overfitting,which helps to reduce the variance and improve the generalization of the ensemble predictions. \n",
    "\n",
    "  **Computentional cost :** But when the ensemble size is increased the computentional cost will also increased. as it takes more prosecing according to the base learners numbers.\n",
    "  \n",
    "   However there is no any limit of using the base learners it is completely depends on the conditions and situations of the data.The optimal ensemble size depends on various factors, including the complexity of the problem, the size of the training data, the nature of the data, and available computational resources.By using cross-validation method , we determine the best ensamble size for our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2f6de-ab2e-4eed-9142-197d112b63ad",
   "metadata": {},
   "source": [
    "# Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403690e-1894-49e0-aea8-28bc31e1594a",
   "metadata": {},
   "source": [
    "Bagging can be used for the brest cancer data set, to determine ..whether a women have a brest cancer or not. We can use decision tree and forest fire algorithems for that. Decision tree may hept to reduced the overfitting as it makes always a new tree for each sample of the data (and these samples are classified by bootstrap sampling).While Forest fire algorithem can help us to create a ensemble classifier for detection of breast cancer.We can also use the bagging for financial models as for stock price predictions..etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
